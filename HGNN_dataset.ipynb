{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, scatter\n",
    "from torch.nn import Parameter\n",
    "from csv import writer\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#load matrix and correlation\n",
    "def matrix_loader(root):\n",
    "    ts_list = sorted(os.listdir(root))\n",
    "    ts_path_list = []\n",
    "    for i in range(0, len(ts_list)):\n",
    "            ts_path_list.append(os.path.join(root, ts_list[i]))\n",
    "    return ts_path_list\n",
    "\n",
    "def filter_SMC_patient_info():\n",
    "    df          = pd.read_csv('/Users/georgepulickal/Documents/ADNI_FULL/patient_info.csv')\n",
    "    labels      = df['Research Group']\n",
    "    label_idx_list = [i for i in range(len(labels)) if labels[i] != 'SMC']\n",
    "    return label_idx_list\n",
    "\n",
    "def store_results(List):\n",
    "    with open('results.csv', 'a') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow(List)\n",
    "        f_object.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ADNI_gsr_full/hypergraphs/cluster/thresh_0.6'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m corr_list \u001B[38;5;241m=\u001B[39m matrix_loader(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mADNI_gsr_172/corr_matrices\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m hg_list \u001B[38;5;241m=\u001B[39m \u001B[43mmatrix_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mADNI_gsr_full/hypergraphs/cluster/thresh_0.6\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m corr_test \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mloadtxt(corr_list[\u001B[38;5;241m0\u001B[39m], delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m hg_test \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mloadtxt(hg_list[\u001B[38;5;241m0\u001B[39m], delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36mmatrix_loader\u001B[0;34m(root)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmatrix_loader\u001B[39m(root):\n\u001B[0;32m----> 3\u001B[0m     ts_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      4\u001B[0m     ts_path_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(ts_list)):\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'ADNI_gsr_full/hypergraphs/cluster/thresh_0.6'"
     ]
    }
   ],
   "source": [
    "corr_list = matrix_loader('ADNI_gsr_172/corr_matrices')\n",
    "hg_list = matrix_loader('ADNI_gsr_full/hypergraphs/cluster/thresh_0.6')\n",
    "corr_test = np.loadtxt(corr_list[0], delimiter=',')\n",
    "hg_test = np.loadtxt(hg_list[0], delimiter=',')\n",
    "hg_nx = from_numpy_array(hg_test)\n",
    "hg_matrix_data = from_networkx(hg_nx)\n",
    "hg_matrix_data.x = torch.tensor(corr_test).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class HGNN_ADNI_dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, hg_data_path = 'hypergraphs/cluster/thresh_0.6'):\n",
    "        self.hg_data_path = hg_data_path\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Paths of connectivity matrices\n",
    "        full_corr_list = matrix_loader('ADNI_gsr_full/corr_matrices')\n",
    "        idx = filter_SMC_patient_info()\n",
    "        corr_list = [full_corr_list[i] for i in idx]\n",
    "\n",
    "        hg_list   = matrix_loader(self.hg_data_path)\n",
    "        idx = filter_SMC_patient_info()\n",
    "        new_hg_list = [hg_list[i] for i in idx]\n",
    "        labels = torch.from_numpy(np.loadtxt('ADNI_gsr_172/labels.csv', delimiter=','))\n",
    "        assert len(corr_list) == len(new_hg_list)\n",
    "        assert len(labels) == len(corr_list)\n",
    "\n",
    "        graphs = []\n",
    "        for i in range(0, len(corr_list)):\n",
    "            corr_array = np.loadtxt(corr_list[i], delimiter=',')\n",
    "            hg_array = np.loadtxt(new_hg_list[i], delimiter=',')\n",
    "\n",
    "            #Pushing partial correlation matrices through pipeline to get final Data object\n",
    "            hg_nx = from_numpy_array(hg_array)\n",
    "            hg_matrix_data = from_networkx(hg_nx)\n",
    "            hg_matrix_data.x = torch.tensor(corr_array).float()\n",
    "            hg_matrix_data.y = labels[i].type(torch.LongTensor)\n",
    "            #hg_matrix_data.pos = coordinates\n",
    "\n",
    "            # Add to running list of all dataset items\n",
    "            graphs.append(hg_matrix_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: HGNN_ADNI_dataset(172):\n",
      "====================\n",
      "Number of hypergraphs: 172\n",
      "Number of features: 116\n",
      "Number of classes: 3\n",
      "\n",
      "Data(edge_index=[2, 296], weight=[296], x=[116, 116], y=[1], num_nodes=116)\n",
      "=============================================================\n",
      "Number of nodes: 116\n",
      "Number of edges: 296\n",
      "Average node degree: 2.55\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = HGNN_ADNI_dataset('ADNI_gsr_pyg_test_hypergraph_cluster')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of hypergraphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#hypergraph convolution\n",
    "class HyperGraph1(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(HyperGraph1, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.hconv1 = HypergraphConv(dataset.num_node_features, hidden_channels, use_attention=False, heads=1)\n",
    "        self.hconv2 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        #self.conv3 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.hconv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.hconv2(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class HyperGraph2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(HyperGraph2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.hconv1 = HypergraphConv(dataset.num_node_features, hidden_channels, use_attention=True, heads=8, concat=True,bias=True,dropout=0.6)\n",
    "        self.hconv2 = HypergraphConv(hidden_channels * 8, hidden_channels,  use_attention=False, heads=1, concat=True,bias=True,dropout=0.6)\n",
    "        #self.conv3 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.hconv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.hconv2(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.#\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 119\n",
      "Number of validation graphs: 18\n",
      "Number of test graphs: 35\n",
      "Epoch: 001, Train Acc: 0.2689, Valid Acc: 0.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgepulickal/PycharmProjects/Alzheimers/venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Acc: 0.2689, Valid Acc: 0.2778\n",
      "Epoch: 003, Train Acc: 0.5042, Valid Acc: 0.3333\n",
      "Epoch: 004, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 005, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 006, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 007, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 008, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 009, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 010, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 011, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 012, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 013, Train Acc: 0.5630, Valid Acc: 0.2778\n",
      "Epoch: 014, Train Acc: 0.5882, Valid Acc: 0.2778\n",
      "Epoch: 015, Train Acc: 0.6050, Valid Acc: 0.2778\n",
      "Epoch: 016, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 017, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 018, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 019, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 020, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 021, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 022, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 023, Train Acc: 0.6134, Valid Acc: 0.2778\n",
      "Epoch: 024, Train Acc: 0.6303, Valid Acc: 0.2778\n",
      "Epoch: 025, Train Acc: 0.6218, Valid Acc: 0.2778\n",
      "Epoch: 026, Train Acc: 0.6303, Valid Acc: 0.2778\n",
      "Epoch: 027, Train Acc: 0.6303, Valid Acc: 0.2778\n",
      "Epoch: 028, Train Acc: 0.6387, Valid Acc: 0.2778\n",
      "Epoch: 029, Train Acc: 0.6639, Valid Acc: 0.2778\n",
      "Test Acc:  0.6286\n",
      "Number of training graphs: 119\n",
      "Number of validation graphs: 18\n",
      "Number of test graphs: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgepulickal/PycharmProjects/Alzheimers/venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2689, Valid Acc: 0.2222\n",
      "Epoch: 002, Train Acc: 0.2773, Valid Acc: 0.2222\n",
      "Epoch: 003, Train Acc: 0.4538, Valid Acc: 0.3333\n",
      "Epoch: 004, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 005, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 006, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 007, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 008, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 009, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 010, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 011, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 012, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 013, Train Acc: 0.5882, Valid Acc: 0.4444\n",
      "Epoch: 014, Train Acc: 0.5966, Valid Acc: 0.4444\n",
      "Epoch: 015, Train Acc: 0.5966, Valid Acc: 0.4444\n",
      "Epoch: 016, Train Acc: 0.6050, Valid Acc: 0.4444\n",
      "Epoch: 017, Train Acc: 0.6134, Valid Acc: 0.4444\n",
      "Epoch: 018, Train Acc: 0.6134, Valid Acc: 0.4444\n",
      "Epoch: 019, Train Acc: 0.6050, Valid Acc: 0.4444\n",
      "Epoch: 020, Train Acc: 0.6134, Valid Acc: 0.4444\n",
      "Epoch: 021, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 022, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 023, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 024, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 025, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 026, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 027, Train Acc: 0.6218, Valid Acc: 0.4444\n",
      "Epoch: 028, Train Acc: 0.6471, Valid Acc: 0.4444\n",
      "Epoch: 029, Train Acc: 0.6555, Valid Acc: 0.3889\n",
      "Test Acc:  0.4857\n",
      "Number of training graphs: 120\n",
      "Number of validation graphs: 18\n",
      "Number of test graphs: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgepulickal/PycharmProjects/Alzheimers/venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2500, Valid Acc: 0.3889\n",
      "Epoch: 002, Train Acc: 0.2583, Valid Acc: 0.3889\n",
      "Epoch: 003, Train Acc: 0.4167, Valid Acc: 0.3889\n",
      "Epoch: 004, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 005, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 006, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 007, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 008, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 009, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 010, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 011, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 012, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 013, Train Acc: 0.5750, Valid Acc: 0.4444\n",
      "Epoch: 014, Train Acc: 0.5833, Valid Acc: 0.4444\n",
      "Epoch: 015, Train Acc: 0.5917, Valid Acc: 0.4444\n",
      "Epoch: 016, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 017, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 018, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 019, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 020, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 021, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 022, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 023, Train Acc: 0.6083, Valid Acc: 0.4444\n",
      "Epoch: 024, Train Acc: 0.6167, Valid Acc: 0.4444\n",
      "Epoch: 025, Train Acc: 0.6333, Valid Acc: 0.5000\n",
      "Epoch: 026, Train Acc: 0.6333, Valid Acc: 0.5000\n",
      "Epoch: 027, Train Acc: 0.6333, Valid Acc: 0.5000\n",
      "Epoch: 028, Train Acc: 0.6333, Valid Acc: 0.5000\n",
      "Epoch: 029, Train Acc: 0.6333, Valid Acc: 0.5000\n",
      "Test Acc:  0.5294\n",
      "Number of training graphs: 120\n",
      "Number of validation graphs: 18\n",
      "Number of test graphs: 34\n",
      "Epoch: 001, Train Acc: 0.3000, Valid Acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgepulickal/PycharmProjects/Alzheimers/venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Acc: 0.3083, Valid Acc: 0.3333\n",
      "Epoch: 003, Train Acc: 0.4167, Valid Acc: 0.3333\n",
      "Epoch: 004, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 005, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 006, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 007, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 008, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 009, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 010, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 011, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 012, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 013, Train Acc: 0.5333, Valid Acc: 0.5000\n",
      "Epoch: 014, Train Acc: 0.5417, Valid Acc: 0.5000\n",
      "Epoch: 015, Train Acc: 0.5417, Valid Acc: 0.5000\n",
      "Epoch: 016, Train Acc: 0.5500, Valid Acc: 0.5000\n",
      "Epoch: 017, Train Acc: 0.5500, Valid Acc: 0.5000\n",
      "Epoch: 018, Train Acc: 0.5500, Valid Acc: 0.5000\n",
      "Epoch: 019, Train Acc: 0.5500, Valid Acc: 0.5000\n",
      "Epoch: 020, Train Acc: 0.5583, Valid Acc: 0.5000\n",
      "Epoch: 021, Train Acc: 0.5750, Valid Acc: 0.5000\n",
      "Epoch: 022, Train Acc: 0.5750, Valid Acc: 0.5000\n",
      "Epoch: 023, Train Acc: 0.6167, Valid Acc: 0.5000\n",
      "Epoch: 024, Train Acc: 0.6667, Valid Acc: 0.5000\n",
      "Epoch: 025, Train Acc: 0.7167, Valid Acc: 0.5000\n",
      "Epoch: 026, Train Acc: 0.7167, Valid Acc: 0.5000\n",
      "Epoch: 027, Train Acc: 0.7000, Valid Acc: 0.5000\n",
      "Epoch: 028, Train Acc: 0.7000, Valid Acc: 0.4444\n",
      "Epoch: 029, Train Acc: 0.7000, Valid Acc: 0.4444\n",
      "Test Acc:  0.6471\n",
      "Number of training graphs: 120\n",
      "Number of validation graphs: 18\n",
      "Number of test graphs: 34\n",
      "Epoch: 001, Train Acc: 0.2583, Valid Acc: 0.2778\n",
      "Epoch: 002, Train Acc: 0.2667, Valid Acc: 0.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgepulickal/PycharmProjects/Alzheimers/venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Acc: 0.4250, Valid Acc: 0.3889\n",
      "Epoch: 004, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 005, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 006, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 007, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 008, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 009, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 010, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 011, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 012, Train Acc: 0.5500, Valid Acc: 0.5556\n",
      "Epoch: 013, Train Acc: 0.5583, Valid Acc: 0.5556\n",
      "Epoch: 014, Train Acc: 0.5750, Valid Acc: 0.5556\n",
      "Epoch: 015, Train Acc: 0.5833, Valid Acc: 0.5556\n",
      "Epoch: 016, Train Acc: 0.5833, Valid Acc: 0.5556\n",
      "Epoch: 017, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 018, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 019, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 020, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 021, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 022, Train Acc: 0.6000, Valid Acc: 0.5000\n",
      "Epoch: 023, Train Acc: 0.6083, Valid Acc: 0.5000\n",
      "Epoch: 024, Train Acc: 0.6167, Valid Acc: 0.5556\n",
      "Epoch: 025, Train Acc: 0.6333, Valid Acc: 0.5556\n",
      "Epoch: 026, Train Acc: 0.6500, Valid Acc: 0.5556\n",
      "Epoch: 027, Train Acc: 0.6500, Valid Acc: 0.5556\n",
      "Epoch: 028, Train Acc: 0.6500, Valid Acc: 0.5556\n",
      "Epoch: 029, Train Acc: 0.6583, Valid Acc: 0.5556\n",
      "Test Acc:  0.5294\n",
      "Average Test Accuracy: 0.5640336134453781\n",
      "Max test accuracy: 0.6470588235294118\n",
      "Standard Deviation: 0.06259325364170408\n"
     ]
    }
   ],
   "source": [
    "tot_test_acc = []\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "for train_val_idx, test_idx in kf.split(dataset, dataset.data.y):\n",
    "    X_train_val = [dataset[i] for i in train_val_idx]\n",
    "    X_test      = [dataset[i] for i in test_idx]\n",
    "    Y_train_val = [dataset.data.y[i] for i in train_val_idx]\n",
    "    Y_test      = [dataset.data.y[i] for i in test_idx]\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_val, Y_train_val , test_size=0.125,\n",
    "                                                    random_state=42, stratify=Y_train_val)\n",
    "\n",
    "    print(f'Number of training graphs: {len(X_train)}')\n",
    "    print(f'Number of validation graphs: {len(X_valid)}')\n",
    "    print(f'Number of test graphs: {len(X_test)}')\n",
    "\n",
    "    train_loader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(X_valid, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(X_test, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = HyperGraph1(hidden_channels=8)\n",
    "    #model = HyperGraph2(hidden_channels=8)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 30):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        valid_acc = test(valid_loader)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "        #wandb.log({\"val_acc\": valid_acc , \"train_acc\": train_acc})\n",
    "\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Test Acc: {test_acc: .4f}')\n",
    "    tot_test_acc.append(test_acc)\n",
    "\n",
    "print(f'Average Test Accuracy: {sum(tot_test_acc) / len(tot_test_acc)}')\n",
    "print(f'Max test accuracy: {max(tot_test_acc)}')\n",
    "print(f'Standard Deviation: {np.std(tot_test_acc)}')\n",
    "results = [dataset.hg_data_path, (sum(tot_test_acc) / len(tot_test_acc)) , np.std(tot_test_acc)]\n",
    "store_results(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
