{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets, plotting, image\n",
    "import nibabel as nib\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, BatchNorm1d\n",
    "from torch_geometric.nn import EdgeConv, GCNConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "dataset_path = 'ADNI_gsr'\n",
    "corr_matrices_dir = f'{dataset_path}/corr_matrices'\n",
    "pcorr_matrices_dir = f'{dataset_path}/pcorr_matrices'\n",
    "avg_pcorr_file = f'{dataset_path}/avg_pcorr.csv'\n",
    "time_series_dir = f'{dataset_path}/time_series'\n",
    "labels_file = f'{dataset_path}/labels.csv'\n",
    "\n",
    "atlas = datasets.fetch_atlas_aal()\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas.maps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class ADNI_dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, neighbors=10):\n",
    "        self.neighbors = neighbors\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Paths of connectivity matrices\n",
    "        corr_path_list = sorted(os.listdir(corr_matrices_dir))\n",
    "        pcorr_path_list = sorted(os.listdir(pcorr_matrices_dir))\n",
    "\n",
    "        graphs = []\n",
    "        labels = torch.from_numpy(np.loadtxt(labels_file, delimiter=','))\n",
    "\n",
    "        for i in range(0, len(corr_path_list)):\n",
    "            corr_matrix_path = os.path.join(corr_matrices_dir, corr_path_list[i])\n",
    "            pcorr_matrix_path = os.path.join(pcorr_matrices_dir, pcorr_path_list[i])\n",
    "\n",
    "            #Pushing partial correlation matrices through pipeline to get final Data object\n",
    "            pcorr_matrix_np = np.loadtxt(pcorr_matrix_path, delimiter=',')\n",
    "\n",
    "            index = np.abs(pcorr_matrix_np).argsort(axis=1)\n",
    "            n_rois = pcorr_matrix_np.shape[0]\n",
    "\n",
    "            # Take only the top k correlates to reduce number of edges\n",
    "            for j in range(n_rois):\n",
    "                for k in range(n_rois - self.neighbors):\n",
    "                    pcorr_matrix_np[j, index[j, k]] = 0\n",
    "                for k in range(n_rois - self.neighbors, n_rois):\n",
    "                    pcorr_matrix_np[j, index[j, k]] = 1\n",
    "\n",
    "            pcorr_matrix_nx = from_numpy_array(pcorr_matrix_np)\n",
    "            pcorr_matrix_data = from_networkx(pcorr_matrix_nx)\n",
    "\n",
    "            # Correlation matrix which will serve as our features\n",
    "            corr_matrix_np = np.loadtxt(corr_matrix_path, delimiter=',')\n",
    "\n",
    "            pcorr_matrix_data.x = torch.tensor(corr_matrix_np).float()\n",
    "            pcorr_matrix_data.y = labels[i].type(torch.LongTensor)\n",
    "            #pcorr_matrix_data.pos = coordinates\n",
    "\n",
    "            # Add to running list of all dataset items\n",
    "            graphs.append(pcorr_matrix_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: ADNI_dataset(172):\n",
      "====================\n",
      "Number of graphs: 172\n",
      "Number of features: 116\n",
      "Number of classes: 3\n",
      "\n",
      "Data(edge_index=[2, 1318], weight=[1318], x=[116, 116], y=[1], pos=[116, 3], num_nodes=116)\n",
      "=============================================================\n",
      "Number of nodes: 116\n",
      "Number of edges: 1318\n",
      "Average node degree: 11.36\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = ADNI_dataset('ADNI_gsr_pyg')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tot_test_acc \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mshuffle()\n\u001B[1;32m      3\u001B[0m X_train_val, X_test, Y_train_val, Y_test \u001B[38;5;241m=\u001B[39m train_test_split(dataset, dataset\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39my , test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[1;32m      4\u001B[0m                                                     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39my)\n",
      "Cell \u001B[0;32mIn[65], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tot_test_acc \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mshuffle()\n\u001B[1;32m      3\u001B[0m X_train_val, X_test, Y_train_val, Y_test \u001B[38;5;241m=\u001B[39m train_test_split(dataset, dataset\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39my , test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[1;32m      4\u001B[0m                                                     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39my)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tot_test_acc = []\n",
    "dataset = dataset.shuffle()\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(dataset, dataset.data.y , test_size=0.2,\n",
    "                                                    random_state=42, stratify=dataset.data.y)\n",
    "kf = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "for train_idx, valid_idx in kf.split(X_train_val, Y_train_val):\n",
    "    X_train = [X_train_val[i] for i in train_idx]\n",
    "    X_valid = [X_train_val[i] for i in valid_idx]\n",
    "    Y_train = [Y_train_val[i] for i in train_idx]\n",
    "    Y_valid = [Y_train_val[i] for i in valid_idx]\n",
    "\n",
    "    print(f'Number of training graphs: {len(X_train)}')\n",
    "    print(f'Number of validation graphs: {len(X_valid)}')\n",
    "    print(f'Number of test graphs: {len(X_test)}')\n",
    "\n",
    "    train_loader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(X_valid, batch_size=10, shuffle=True)\n",
    "    test_loader = DataLoader(X_test, batch_size=10, shuffle=False)\n",
    "\n",
    "    model = GCN(hidden_channels=64)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 171):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        valid_acc = test(valid_loader)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Test Acc: {test_acc: .4f}')\n",
    "    tot_test_acc.append(test_acc)\n",
    "\n",
    "print(f'Average Test Accuracy: {sum(tot_test_acc) / len(tot_test_acc)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
